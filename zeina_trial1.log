============================= test session starts ==============================
platform darwin -- Python 3.10.4, pytest-7.1.2, pluggy-1.0.0 -- /Users/migeedz/opt/anaconda3/envs/pytorch2/bin/python
cachedir: .pytest_cache
rootdir: /Users/migeedz/torchdynamo, configfile: pytest.ini
collecting ... collected 5 items / 4 deselected / 1 selected

tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM ERROR FROM offset=10 filename /Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py 171 AttributeError
========== TorchDynamo Stack Trace ==========
Traceback (most recent call last):
  File "/Users/migeedz/torchdynamo/torchdynamo/convert_frame.py", line 288, in _convert_frame_assert
    code = transform_code_object(frame.f_code, transform)
  File "/Users/migeedz/torchdynamo/torchdynamo/bytecode_transformation.py", line 338, in transform_code_object
    transformations(instructions, code_options)
  File "/Users/migeedz/torchdynamo/torchdynamo/convert_frame.py", line 264, in transform
    tracer.run()
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 312, in run
    and self.step()
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 290, in step
    getattr(self, inst.opname)(inst)
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 151, in wrapper
    return inner_fn(self, inst)
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 627, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 226, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/Users/migeedz/torchdynamo/torchdynamo/variables/misc.py", line 505, in call_function
    return self.obj.call_method(tx, self.name, args, kwargs).add_options(self)
  File "/Users/migeedz/torchdynamo/torchdynamo/variables/nn_module.py", line 460, in call_method
    if id(method.__code__) in self._nn_module_method_ids():
AttributeError: 'staticmethod' object has no attribute '__code__'
========== Exception (above) while processing ==========
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/bin/pytest", line 8, in <module>
    sys.exit(console_main())
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/config/__init__.py", line 187, in console_main
    code = main()
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/config/__init__.py", line 164, in main
    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_hooks.py", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_manager.py", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_callers.py", line 39, in _multicall
    res = hook_impl.function(*args)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/main.py", line 315, in pytest_cmdline_main
    return wrap_session(config, _main)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/main.py", line 268, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/main.py", line 322, in _main
    config.hook.pytest_runtestloop(session=session)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_hooks.py", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_manager.py", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_callers.py", line 39, in _multicall
    res = hook_impl.function(*args)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/main.py", line 347, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_hooks.py", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_manager.py", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_callers.py", line 39, in _multicall
    res = hook_impl.function(*args)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 111, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 130, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 219, in call_and_report
    call = call_runtest_hook(item, when, **kwds)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 258, in call_runtest_hook
    return CallInfo.from_call(
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 338, in from_call
    result: Optional[TResult] = func()
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 259, in <lambda>
    lambda: ihook(item=item, **kwds), when=when, reraise=reraise
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_hooks.py", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_manager.py", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_callers.py", line 39, in _multicall
    res = hook_impl.function(*args)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 166, in pytest_runtest_call
    item.runtest()
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/unittest.py", line 327, in runtest
    self._testcase(result=self)  # type: ignore[arg-type]
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/unittest/case.py", line 650, in __call__
    return self.run(*args, **kwds)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/Users/migeedz/torchdynamo/tests/test_gradual_types.py", line 229, in test_XGLM
    m = generate_hf_model(XGLMModel, hidden_layers=1)
  File "/Users/migeedz/torchdynamo/tests/test_gradual_types.py", line 67, in generate_hf_model
    model = model_cls(config)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 553, in __init__
    self.embed_positions = XGLMSinusoidalPositionalEmbedding(
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 168, in __init__
    self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 170, in make_weights
    def make_weights(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):
========== End debug info ==========
ERROR FROM offset=54 filename /Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py 453 AssertionError
ERROR FROM offset=438 filename /Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py 336 AssertionError
--> GraphModule()



def forward(self):
    arange = torch.arange(512, dtype = torch.float32)
    mul = arange * -0.01802414945592208;  arange = None
    exp = torch.exp(mul);  mul = None
    arange_1 = torch.arange(2050, dtype = torch.float32)
    unsqueeze = arange_1.unsqueeze(1);  arange_1 = None
    unsqueeze_1 = exp.unsqueeze(0);  exp = None
    mul_1 = unsqueeze * unsqueeze_1;  unsqueeze = unsqueeze_1 = None
    sin = torch.sin(mul_1)
    cos = torch.cos(mul_1);  mul_1 = None
    cat = torch.cat([sin, cos], dim = 1);  sin = cos = None
    view = cat.view(2050, -1);  cat = None
    view[(1, slice(None, None, None))] = 0;  setitem = view
    return (view,)
    
--> GraphModule()



def forward(self, _stack0 : torch.Tensor):
    zero_ = _stack0.zero_();  _stack0 = None
    return ()
    
*************************
0
GraphModule()



def forward(self, input_ids : typing_Union[torch.Tensor,NoneType], self_embed_tokens_weight : torch.nn.parameter.Parameter):
    size = input_ids.size()
    getitem = size[-1]
    view = input_ids.view(-1, getitem);  input_ids = getitem = None
    embedding = torch.nn.functional.embedding(view, self_embed_tokens_weight, 1, None, 2.0, False, False);  view = self_embed_tokens_weight = None
    mul = embedding * 32.0;  embedding = None
    getitem_1 = size[-1];  size = None
    gt = getitem_1 > 1;  getitem_1 = None
    
sat
unsat
*************************
1
GraphModule()



def forward(self, input_ids : torch.fx.tensor_type._DynType, self_embed_tokens_weight : torch.fx.tensor_type.TensorType[_256008,_1024]):
    size : torch.fx.tensor_type._DynType = input_ids.size()
    getitem : torch.fx.tensor_type._DynType = size[-1]
    view : torch.fx.tensor_type._DynType = input_ids.view(-1, getitem);  input_ids = getitem = None
    embedding : torch.fx.tensor_type._DynType = torch.nn.functional.embedding(view, self_embed_tokens_weight, 1, None, 2.0, False, False);  view = self_embed_tokens_weight = None
    mul : torch.fx.tensor_type._DynType = embedding * 32.0;  embedding = None
    getitem_1 : torch.fx.tensor_type._DynType = size[-1]
    getitem_2 = size[1]
    getitem_3 = size[0];  size = None
    tensor = torch.tensor(-3.4028234663852886e+38)
    full = torch.full((getitem_2, getitem_2), tensor);  getitem_2 = tensor = None
    size_1 = full.size(-1)
    arange = torch.arange(size_1);  size_1 = None
    add = arange + 1
    size_2 = full.size(-1);  full = None
    view_1 = add.view(size_2, 1);  add = size_2 = None
    lt = arange < view_1;  arange = view_1 = None
    
--> GraphModule()



def forward(self, input_ids : torch.fx.tensor_type._DynType, self_embed_tokens_weight : torch.fx.tensor_type.TensorType[_256008,_1024]):
    size : torch.fx.tensor_type._DynType = input_ids.size()
    getitem : torch.fx.tensor_type._DynType = size[-1]
    view : torch.fx.tensor_type._DynType = input_ids.view(-1, getitem);  input_ids = getitem = None
    embedding : torch.fx.tensor_type._DynType = torch.nn.functional.embedding(view, self_embed_tokens_weight, 1, None, 2.0, False, False);  self_embed_tokens_weight = None
    mul : torch.fx.tensor_type._DynType = embedding * 32.0;  embedding = None
    getitem_2 : torch.fx.tensor_type._DynType = size[1]
    getitem_3 : torch.fx.tensor_type._DynType = size[0];  size = None
    tensor : torch.fx.tensor_type._DynType = torch.tensor(-3.4028234663852886e+38)
    full : torch.fx.tensor_type._DynType = torch.full((getitem_2, getitem_2), tensor);  tensor = None
    size_1 : torch.fx.tensor_type._DynType = full.size(-1)
    arange : torch.fx.tensor_type._DynType = torch.arange(size_1);  size_1 = None
    add : torch.fx.tensor_type._DynType = arange + 1
    size_2 : torch.fx.tensor_type._DynType = full.size(-1)
    view_1 : torch.fx.tensor_type._DynType = add.view(size_2, 1);  add = size_2 = None
    lt : torch.fx.tensor_type._DynType = arange < view_1;  arange = view_1 = None
    masked_fill_ = full.masked_fill_(lt, 0);  lt = None
    to = full.to(torch.float32);  full = None
    getitem_4 = to[(None, None, slice(None, None, None), slice(None, None, None))];  to = None
    add_1 = getitem_2 + 0
    expand = getitem_4.expand(getitem_3, 1, getitem_2, add_1);  getitem_4 = getitem_3 = getitem_2 = add_1 = None
    to_1 = expand.to(device(type='cpu'));  expand = None
    return (view, mul, to_1)
    
*************************
2
GraphModule()



def forward(self, input_ids : torch.Tensor, inputs_embeds : torch.Tensor, self_weights : torch.Tensor):
    size = input_ids.size()
    getitem = size[1]
    getitem_1 = size[0];  size = None
    ne = input_ids.ne(1);  input_ids = None
    int_1 = ne.int();  ne = None
    cumsum = torch.cumsum(int_1, dim = 1)
    type_as = cumsum.type_as(int_1);  cumsum = None
    add = type_as + 0;  type_as = None
    mul = add * int_1;  add = int_1 = None
    long = mul.long();  mul = None
    add_1 = long + 1;  long = None
    to = add_1.to(device(type='cpu'));  add_1 = None
    add_2 = 2 + getitem;  getitem = None
    add_3 = add_2 + 0;  add_2 = None
    size_1 = self_weights.size(0);  self_weights = None
    gt = add_3 > size_1;  add_3 = size_1 = None
    
unsat
sat
--> GraphModule()



def forward(self, input_ids : torch.fx.tensor_type._DynType, self_weights : torch.fx.tensor_type._DynType, self_weights_0 : torch.Tensor):
    size : torch.fx.tensor_type._DynType = input_ids.size()
    getitem : torch.fx.tensor_type._DynType = size[1]
    getitem_1 : torch.fx.tensor_type._DynType = size[0];  size = None
    ne : torch.fx.tensor_type._DynType = input_ids.ne(1);  input_ids = None
    int_1 : torch.fx.tensor_type._DynType = ne.int();  ne = None
    cumsum : torch.fx.tensor_type._DynType = torch.cumsum(int_1, dim = 1)
    type_as : torch.fx.tensor_type._DynType = cumsum.type_as(int_1);  cumsum = None
    add : torch.fx.tensor_type._DynType = type_as + 0;  type_as = None
    mul : torch.fx.tensor_type._DynType = add * int_1;  add = int_1 = None
    long : torch.fx.tensor_type._DynType = mul.long();  mul = None
    add_1 : torch.fx.tensor_type._DynType = long + 1;  long = None
    to : torch.fx.tensor_type._DynType = add_1.to(device(type='cpu'));  add_1 = None
    add_2 : torch.fx.tensor_type._DynType = 2 + getitem
    add_3 : torch.fx.tensor_type._DynType = add_2 + 0;  add_2 = None
    size_1 : torch.fx.tensor_type._DynType = self_weights.size(0);  self_weights = None
    view = to.view(-1);  to = None
    index_select = self_weights_0.index_select(0, view);  self_weights_0 = view = None
    view_1 = index_select.view(getitem_1, getitem, -1);  index_select = getitem_1 = getitem = None
    detach = view_1.detach();  view_1 = None
    return (detach,)
    
*************************
3
GraphModule()



def forward(self, _stack0 : torch.Tensor, attention_mask : torch.Tensor, inputs_embeds : torch.Tensor, random_value_0 : torch.Tensor, self_layers_0_self_attn_layer_norm_weight : torch.nn.parameter.Parameter, self_layers_0_self_attn_layer_norm_bias : torch.nn.parameter.Parameter, self_layers_0_self_attn_q_proj_weight : torch.nn.parameter.Parameter, self_layers_0_self_attn_q_proj_bias : torch.nn.parameter.Parameter, self_layers_0_self_attn_k_proj_weight : torch.nn.parameter.Parameter, self_layers_0_self_attn_k_proj_bias : torch.nn.parameter.Parameter, self_layers_0_self_attn_v_proj_weight : torch.nn.parameter.Parameter, self_layers_0_self_attn_v_proj_bias : torch.nn.parameter.Parameter):
    add = inputs_embeds + _stack0;  inputs_embeds = _stack0 = None
    dropout = torch.nn.functional.dropout(add, p = 0.1, training = False);  add = None
    layer_norm = torch.nn.functional.layer_norm(dropout, (1024,), self_layers_0_self_attn_layer_norm_weight, self_layers_0_self_attn_layer_norm_bias, 1e-05);  dropout = self_layers_0_self_attn_layer_norm_weight = self_layers_0_self_attn_layer_norm_bias = None
    size = layer_norm.size()
    getitem = size[2]
    getitem_1 = size[1]
    getitem_2 = size[0];  size = None
    linear = torch._C._nn.linear(layer_norm, self_layers_0_self_attn_q_proj_weight, self_layers_0_self_attn_q_proj_bias);  self_layers_0_self_attn_q_proj_weight = self_layers_0_self_attn_q_proj_bias = None
    mul = linear * 0.125;  linear = None
    linear_1 = torch._C._nn.linear(layer_norm, self_layers_0_self_attn_k_proj_weight, self_layers_0_self_attn_k_proj_bias);  self_layers_0_self_attn_k_proj_weight = self_layers_0_self_attn_k_proj_bias = None
    view = linear_1.view(getitem_2, -1, 16, 64);  linear_1 = None
    transpose = view.transpose(1, 2);  view = None
    contiguous = transpose.contiguous();  transpose = None
    linear_2 = torch._C._nn.linear(layer_norm, self_layers_0_self_attn_v_proj_weight, self_layers_0_self_attn_v_proj_bias);  layer_norm = self_layers_0_self_attn_v_proj_weight = self_layers_0_self_attn_v_proj_bias = None
    view_1 = linear_2.view(getitem_2, -1, 16, 64);  linear_2 = None
    transpose_1 = view_1.transpose(1, 2);  view_1 = None
    contiguous_1 = transpose_1.contiguous();  transpose_1 = None
    mul_1 = getitem_2 * 16
    view_2 = mul.view(getitem_2, getitem_1, 16, 64);  mul = None
    transpose_2 = view_2.transpose(1, 2);  view_2 = None
    contiguous_2 = transpose_2.contiguous();  transpose_2 = None
    view_3 = contiguous_2.view(mul_1, -1, 64);  contiguous_2 = None
    view_4 = contiguous.view(mul_1, -1, 64);  contiguous = None
    view_5 = contiguous_1.view(mul_1, -1, 64);  contiguous_1 = mul_1 = None
    size_1 = view_4.size(1)
    transpose_3 = view_4.transpose(1, 2);  view_4 = None
    bmm = torch.bmm(view_3, transpose_3);  view_3 = transpose_3 = None
    size_2 = bmm.size();  bmm = None
    mul_2 = getitem_2 * 16;  getitem_2 = None
    ne = size_2 != (mul_2, getitem_1, size_1);  size_2 = mul_2 = getitem_1 = size_1 = None
    
unsat
sat
*************************
4
--> GraphModule()



def forward(self, _stack0 : torch.fx.tensor_type._DynType, inputs_embeds : torch.fx.tensor_type._DynType, random_value_0 : torch.fx.tensor_type._DynType):
    add : torch.fx.tensor_type._DynType = inputs_embeds + _stack0;  inputs_embeds = _stack0 = None
    dropout : torch.fx.tensor_type._DynType = torch.nn.functional.dropout(add, p = 0.1, training = False);  add = None
    return (dropout, random_value_0)
    
*************************
5
GraphModule()



def forward(self, hidden_states : torch.Tensor, attention_mask : typing_Union[torch.Tensor,NoneType], self_self_attn_layer_norm_weight : torch.nn.parameter.Parameter, self_self_attn_layer_norm_bias : torch.nn.parameter.Parameter, self_self_attn_q_proj_weight : torch.nn.parameter.Parameter, self_self_attn_q_proj_bias : torch.nn.parameter.Parameter, self_self_attn_k_proj_weight : torch.nn.parameter.Parameter, self_self_attn_k_proj_bias : torch.nn.parameter.Parameter, self_self_attn_v_proj_weight : torch.nn.parameter.Parameter, self_self_attn_v_proj_bias : torch.nn.parameter.Parameter):
    layer_norm = torch.nn.functional.layer_norm(hidden_states, (1024,), self_self_attn_layer_norm_weight, self_self_attn_layer_norm_bias, 1e-05);  hidden_states = self_self_attn_layer_norm_weight = self_self_attn_layer_norm_bias = None
    size = layer_norm.size()
    getitem = size[2]
    getitem_1 = size[1]
    getitem_2 = size[0];  size = None
    linear = torch._C._nn.linear(layer_norm, self_self_attn_q_proj_weight, self_self_attn_q_proj_bias);  self_self_attn_q_proj_weight = self_self_attn_q_proj_bias = None
    mul = linear * 0.125;  linear = None
    linear_1 = torch._C._nn.linear(layer_norm, self_self_attn_k_proj_weight, self_self_attn_k_proj_bias);  self_self_attn_k_proj_weight = self_self_attn_k_proj_bias = None
    view = linear_1.view(getitem_2, -1, 16, 64);  linear_1 = None
    transpose = view.transpose(1, 2);  view = None
    contiguous = transpose.contiguous();  transpose = None
    linear_2 = torch._C._nn.linear(layer_norm, self_self_attn_v_proj_weight, self_self_attn_v_proj_bias);  layer_norm = self_self_attn_v_proj_weight = self_self_attn_v_proj_bias = None
    view_1 = linear_2.view(getitem_2, -1, 16, 64);  linear_2 = None
    transpose_1 = view_1.transpose(1, 2);  view_1 = None
    contiguous_1 = transpose_1.contiguous();  transpose_1 = None
    mul_1 = getitem_2 * 16
    view_2 = mul.view(getitem_2, getitem_1, 16, 64);  mul = None
    transpose_2 = view_2.transpose(1, 2);  view_2 = None
    contiguous_2 = transpose_2.contiguous();  transpose_2 = None
    view_3 = contiguous_2.view(mul_1, -1, 64);  contiguous_2 = None
    view_4 = contiguous.view(mul_1, -1, 64);  contiguous = None
    view_5 = contiguous_1.view(mul_1, -1, 64);  contiguous_1 = mul_1 = None
    size_1 = view_4.size(1)
    transpose_3 = view_4.transpose(1, 2);  view_4 = None
    bmm = torch.bmm(view_3, transpose_3);  view_3 = transpose_3 = None
    size_2 = bmm.size();  bmm = None
    mul_2 = getitem_2 * 16;  getitem_2 = None
    ne = size_2 != (mul_2, getitem_1, size_1);  size_2 = mul_2 = getitem_1 = size_1 = None
    
unsat
sat
*************************
6
*************************
7
GraphModule()



def forward(self, hidden_states : torch.Tensor, attention_mask : typing_Union[torch.Tensor,NoneType], self_q_proj_weight : torch.nn.parameter.Parameter, self_q_proj_bias : torch.nn.parameter.Parameter, self_k_proj_weight : torch.nn.parameter.Parameter, self_k_proj_bias : torch.nn.parameter.Parameter, self_v_proj_weight : torch.nn.parameter.Parameter, self_v_proj_bias : torch.nn.parameter.Parameter):
    size = hidden_states.size()
    getitem = size[2]
    getitem_1 = size[1]
    getitem_2 = size[0];  size = None
    linear = torch._C._nn.linear(hidden_states, self_q_proj_weight, self_q_proj_bias);  self_q_proj_weight = self_q_proj_bias = None
    mul = linear * 0.125;  linear = None
    linear_1 = torch._C._nn.linear(hidden_states, self_k_proj_weight, self_k_proj_bias);  self_k_proj_weight = self_k_proj_bias = None
    view = linear_1.view(getitem_2, -1, 16, 64);  linear_1 = None
    transpose = view.transpose(1, 2);  view = None
    contiguous = transpose.contiguous();  transpose = None
    linear_2 = torch._C._nn.linear(hidden_states, self_v_proj_weight, self_v_proj_bias);  hidden_states = self_v_proj_weight = self_v_proj_bias = None
    view_1 = linear_2.view(getitem_2, -1, 16, 64);  linear_2 = None
    transpose_1 = view_1.transpose(1, 2);  view_1 = None
    contiguous_1 = transpose_1.contiguous();  transpose_1 = None
    mul_1 = getitem_2 * 16
    view_2 = mul.view(getitem_2, getitem_1, 16, 64);  mul = None
    transpose_2 = view_2.transpose(1, 2);  view_2 = None
    contiguous_2 = transpose_2.contiguous();  transpose_2 = None
    view_3 = contiguous_2.view(mul_1, -1, 64);  contiguous_2 = None
    view_4 = contiguous.view(mul_1, -1, 64);  contiguous = None
    view_5 = contiguous_1.view(mul_1, -1, 64);  contiguous_1 = mul_1 = None
    size_1 = view_4.size(1)
    transpose_3 = view_4.transpose(1, 2);  view_4 = None
    bmm = torch.bmm(view_3, transpose_3);  view_3 = transpose_3 = None
    size_2 = bmm.size();  bmm = None
    mul_2 = getitem_2 * 16;  getitem_2 = None
    ne = size_2 != (mul_2, getitem_1, size_1);  size_2 = mul_2 = getitem_1 = size_1 = None
    
unsat
sat
*************************
8
--> GraphModule()



def forward(self, tensor : torch.Tensor):
    view = tensor.view(4, -1, 16, 64);  tensor = None
    transpose = view.transpose(1, 2);  view = None
    contiguous = transpose.contiguous();  transpose = None
    return (contiguous,)
    
--> GraphModule()



def forward(self, tensor : torch.Tensor):
    view = tensor.view(4, 32, 16, 64);  tensor = None
    transpose = view.transpose(1, 2);  view = None
    contiguous = transpose.contiguous();  transpose = None
    return (contiguous,)
    
--> GraphModule()



def forward(self, input : torch.Tensor):
    input_1 = input
    gelu = torch._C._nn.gelu(input_1);  input_1 = None
    return (gelu,)
    
--> GraphModule()



def forward(self, _stack1_0_ : torch.Tensor, self_layer_norm_weight : torch.nn.parameter.Parameter, self_layer_norm_bias : torch.nn.parameter.Parameter):
    layer_norm = torch.nn.functional.layer_norm(_stack1_0_, (1024,), self_layer_norm_weight, self_layer_norm_bias, 1e-05);  _stack1_0_ = self_layer_norm_weight = self_layer_norm_bias = None
    return (layer_norm,)
    
Nuum_graphs 9
PASSED

=============================== warnings summary ===============================
<frozen importlib._bootstrap>:283
  <frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead

tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM
tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM
tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM
tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM
tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM
tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM
tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM
tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM
  /Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:141: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/migeedz/pytorch/build/aten/src/ATen/core/TensorBody.h:483.)
    if t.grad is not None:

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================= 1 passed, 4 deselected, 9 warnings in 14.81s =================
