============================= test session starts ==============================
platform darwin -- Python 3.10.4, pytest-7.1.2, pluggy-1.0.0 -- /Users/migeedz/opt/anaconda3/envs/pytorch2/bin/python
cachedir: .pytest_cache
rootdir: /Users/migeedz/torchdynamo, configfile: pytest.ini
collecting ... collected 5 items / 4 deselected / 1 selected

tests/test_gradual_types.py::TorchDynamoUseCases::test_XGLM ERROR FROM offset=10 filename /Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py 171 AttributeError
========== TorchDynamo Stack Trace ==========
Traceback (most recent call last):
  File "/Users/migeedz/torchdynamo/torchdynamo/convert_frame.py", line 288, in _convert_frame_assert
    code = transform_code_object(frame.f_code, transform)
  File "/Users/migeedz/torchdynamo/torchdynamo/bytecode_transformation.py", line 338, in transform_code_object
    transformations(instructions, code_options)
  File "/Users/migeedz/torchdynamo/torchdynamo/convert_frame.py", line 264, in transform
    tracer.run()
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 312, in run
    and self.step()
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 290, in step
    getattr(self, inst.opname)(inst)
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 151, in wrapper
    return inner_fn(self, inst)
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 627, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/Users/migeedz/torchdynamo/torchdynamo/symbolic_convert.py", line 226, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/Users/migeedz/torchdynamo/torchdynamo/variables/misc.py", line 505, in call_function
    return self.obj.call_method(tx, self.name, args, kwargs).add_options(self)
  File "/Users/migeedz/torchdynamo/torchdynamo/variables/nn_module.py", line 460, in call_method
    if id(method.__code__) in self._nn_module_method_ids():
AttributeError: 'staticmethod' object has no attribute '__code__'
========== Exception (above) while processing ==========
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/bin/pytest", line 8, in <module>
    sys.exit(console_main())
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/config/__init__.py", line 187, in console_main
    code = main()
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/config/__init__.py", line 164, in main
    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_hooks.py", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_manager.py", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_callers.py", line 39, in _multicall
    res = hook_impl.function(*args)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/main.py", line 315, in pytest_cmdline_main
    return wrap_session(config, _main)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/main.py", line 268, in wrap_session
    session.exitstatus = doit(config, session) or 0
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/main.py", line 322, in _main
    config.hook.pytest_runtestloop(session=session)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_hooks.py", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_manager.py", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_callers.py", line 39, in _multicall
    res = hook_impl.function(*args)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/main.py", line 347, in pytest_runtestloop
    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_hooks.py", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_manager.py", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_callers.py", line 39, in _multicall
    res = hook_impl.function(*args)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 111, in pytest_runtest_protocol
    runtestprotocol(item, nextitem=nextitem)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 130, in runtestprotocol
    reports.append(call_and_report(item, "call", log))
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 219, in call_and_report
    call = call_runtest_hook(item, when, **kwds)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 258, in call_runtest_hook
    return CallInfo.from_call(
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 338, in from_call
    result: Optional[TResult] = func()
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 259, in <lambda>
    lambda: ihook(item=item, **kwds), when=when, reraise=reraise
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_hooks.py", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_manager.py", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/pluggy/_callers.py", line 39, in _multicall
    res = hook_impl.function(*args)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/runner.py", line 166, in pytest_runtest_call
    item.runtest()
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/_pytest/unittest.py", line 327, in runtest
    self._testcase(result=self)  # type: ignore[arg-type]
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/unittest/case.py", line 650, in __call__
    return self.run(*args, **kwds)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/unittest/case.py", line 591, in run
    self._callTestMethod(testMethod)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/unittest/case.py", line 549, in _callTestMethod
    method()
  File "/Users/migeedz/torchdynamo/tests/test_gradual_types.py", line 229, in test_XGLM
    m = generate_hf_model(XGLMModel, hidden_layers=1)
  File "/Users/migeedz/torchdynamo/tests/test_gradual_types.py", line 67, in generate_hf_model
    model = model_cls(config)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 553, in __init__
    self.embed_positions = XGLMSinusoidalPositionalEmbedding(
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 168, in __init__
    self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)
  File "/Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 170, in make_weights
    def make_weights(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):
========== End debug info ==========
--> GraphModule()



def forward(self):
    arange = torch.arange(512, dtype = torch.float32)
    mul = arange * -0.01802414945592208;  arange = None
    exp = torch.exp(mul);  mul = None
    arange_1 = torch.arange(2050, dtype = torch.float32)
    unsqueeze = arange_1.unsqueeze(1);  arange_1 = None
    unsqueeze_1 = exp.unsqueeze(0);  exp = None
    mul_1 = unsqueeze * unsqueeze_1;  unsqueeze = unsqueeze_1 = None
    sin = torch.sin(mul_1)
    cos = torch.cos(mul_1);  mul_1 = None
    cat = torch.cat([sin, cos], dim = 1);  sin = cos = None
    view = cat.view(2050, -1);  cat = None
    view[(1, slice(None, None, None))] = 0;  setitem = view
    return (view,)
    
--> GraphModule()



def forward(self, _stack0 : torch.Tensor):
    zero_ = _stack0.zero_();  _stack0 = None
    return ()
    
--> GraphModule()



def forward(self, input_ids : torch.Tensor, self_embed_tokens_weight : torch.nn.parameter.Parameter):
    size = input_ids.size()
    getitem = size[-1]
    view = input_ids.view(-1, getitem);  input_ids = getitem = None
    embedding = torch.nn.functional.embedding(view, self_embed_tokens_weight, 1, None, 2.0, False, False);  self_embed_tokens_weight = None
    mul = embedding * 32.0;  embedding = None
    return (size, mul, view)
    
--> GraphModule()



def forward(self):
    tensor = torch.tensor(-3.4028234663852886e+38)
    full = torch.full((32, 32), tensor);  tensor = None
    size = full.size(-1)
    arange = torch.arange(size);  size = None
    add = arange + 1
    size_1 = full.size(-1)
    view = add.view(size_1, 1);  add = size_1 = None
    lt = arange < view;  arange = view = None
    masked_fill_ = full.masked_fill_(lt, 0);  lt = None
    to = full.to(torch.float32);  full = None
    getitem = to[(None, None, slice(None, None, None), slice(None, None, None))];  to = None
    expand = getitem.expand(4, 1, 32, 32);  getitem = None
    to_1 = expand.to(device(type='cpu'));  expand = None
    return (to_1,)
    
--> GraphModule()



def forward(self, input_ids : torch.Tensor, self_weights : torch.Tensor):
    size = input_ids.size()
    getitem = size[1]
    getitem_1 = size[0];  size = None
    ne = input_ids.ne(1);  input_ids = None
    int_1 = ne.int();  ne = None
    cumsum = torch.cumsum(int_1, dim = 1)
    type_as = cumsum.type_as(int_1);  cumsum = None
    add = type_as + 0;  type_as = None
    mul = add * int_1;  add = int_1 = None
    long = mul.long();  mul = None
    add_1 = long + 1;  long = None
    to = add_1.to(device(type='cpu'));  add_1 = None
    add_2 = 2 + getitem
    add_3 = add_2 + 0;  add_2 = None
    size_1 = self_weights.size(0);  self_weights = None
    gt = add_3 > size_1;  size_1 = None
    return (add_3, to, getitem, getitem_1, gt)
    
--> GraphModule()



def forward(self, position_ids : torch.Tensor, self_weights : torch.Tensor):
    view = position_ids.view(-1);  position_ids = None
    index_select = self_weights.index_select(0, view);  self_weights = view = None
    view_1 = index_select.view(4, 32, -1);  index_select = None
    detach = view_1.detach();  view_1 = None
    return (detach,)
    
--> GraphModule()



def forward(self, _stack0 : torch.Tensor, inputs_embeds : torch.Tensor, random_value_0 : torch.Tensor):
    add = inputs_embeds + _stack0;  inputs_embeds = _stack0 = None
    dropout = torch.nn.functional.dropout(add, p = 0.1, training = False);  add = None
    return (dropout, random_value_0)
    
--> GraphModule()



def forward(self, hidden_states : torch.Tensor, self_self_attn_layer_norm_weight : torch.nn.parameter.Parameter, self_self_attn_layer_norm_bias : torch.nn.parameter.Parameter):
    layer_norm = torch.nn.functional.layer_norm(hidden_states, (1024,), self_self_attn_layer_norm_weight, self_self_attn_layer_norm_bias, 1e-05);  hidden_states = self_self_attn_layer_norm_weight = self_self_attn_layer_norm_bias = None
    return (layer_norm,)
    
--> GraphModule()



def forward(self, hidden_states : torch.Tensor, self_q_proj_weight : torch.nn.parameter.Parameter, self_q_proj_bias : torch.nn.parameter.Parameter, self_k_proj_weight : torch.nn.parameter.Parameter, self_k_proj_bias : torch.nn.parameter.Parameter, self_v_proj_weight : torch.nn.parameter.Parameter, self_v_proj_bias : torch.nn.parameter.Parameter):
    size = hidden_states.size()
    getitem_1 = size[1]
    getitem_2 = size[0];  size = None
    linear = torch._C._nn.linear(hidden_states, self_q_proj_weight, self_q_proj_bias);  self_q_proj_weight = self_q_proj_bias = None
    mul = linear * 0.125;  linear = None
    linear_1 = torch._C._nn.linear(hidden_states, self_k_proj_weight, self_k_proj_bias);  self_k_proj_weight = self_k_proj_bias = None
    view = linear_1.view(getitem_2, -1, 16, 64);  linear_1 = None
    transpose = view.transpose(1, 2);  view = None
    contiguous = transpose.contiguous();  transpose = None
    linear_2 = torch._C._nn.linear(hidden_states, self_v_proj_weight, self_v_proj_bias);  hidden_states = self_v_proj_weight = self_v_proj_bias = None
    view_1 = linear_2.view(getitem_2, -1, 16, 64);  linear_2 = None
    transpose_1 = view_1.transpose(1, 2);  view_1 = None
    contiguous_1 = transpose_1.contiguous();  transpose_1 = None
    mul_1 = getitem_2 * 16
    view_2 = mul.view(getitem_2, getitem_1, 16, 64);  mul = None
    transpose_2 = view_2.transpose(1, 2);  view_2 = None
    contiguous_2 = transpose_2.contiguous();  transpose_2 = None
    view_3 = contiguous_2.view(mul_1, -1, 64);  contiguous_2 = None
    view_4 = contiguous.view(mul_1, -1, 64)
    view_5 = contiguous_1.view(mul_1, -1, 64);  mul_1 = None
    size_1 = view_4.size(1)
    transpose_3 = view_4.transpose(1, 2);  view_4 = None
    bmm = torch.bmm(view_3, transpose_3);  view_3 = transpose_3 = None
    size_2 = bmm.size()
    mul_2 = getitem_2 * 16
    ne = size_2 != (mul_2, getitem_1, size_1);  size_2 = mul_2 = None
    return (ne, contiguous, contiguous_1, getitem_2, getitem_1, view_5, size_1, bmm)
    
--> GraphModule()



def forward(self, attention_mask : torch.Tensor):
    size = attention_mask.size();  attention_mask = None
    ne = size != (4, 1, 32, 32);  size = None
    return (ne,)
    
--> GraphModule()



def forward(self, attention_mask : torch.Tensor, value_states : torch.Tensor, attn_weights : torch.Tensor):
    view = attn_weights.view(4, 16, 32, 32);  attn_weights = None
    add = view + attention_mask;  view = attention_mask = None
    view_1 = add.view(64, 32, 32);  add = None
    softmax = torch.nn.functional.softmax(view_1, dim = -1);  view_1 = None
    dropout = torch.nn.functional.dropout(softmax, p = 0.1, training = False);  softmax = None
    bmm = torch.bmm(dropout, value_states);  dropout = value_states = None
    size = bmm.size()
    ne = size != (64, 32, 64);  size = None
    return (ne, bmm)
    
--> GraphModule()



def forward(self, attn_output : torch.Tensor, self_out_proj_weight : torch.nn.parameter.Parameter, self_out_proj_bias : torch.nn.parameter.Parameter):
    view = attn_output.view(4, 16, 32, 64);  attn_output = None
    transpose = view.transpose(1, 2);  view = None
    reshape = transpose.reshape(4, 32, 1024);  transpose = None
    linear = torch._C._nn.linear(reshape, self_out_proj_weight, self_out_proj_bias);  reshape = self_out_proj_weight = self_out_proj_bias = None
    return (linear,)
    
--> GraphModule()



def forward(self, _stack0_0_ : torch.Tensor, residual : torch.Tensor, self_final_layer_norm_weight : torch.nn.parameter.Parameter, self_final_layer_norm_bias : torch.nn.parameter.Parameter, self_fc1_weight : torch.nn.parameter.Parameter, self_fc1_bias : torch.nn.parameter.Parameter, self_fc2_weight : torch.nn.parameter.Parameter, self_fc2_bias : torch.nn.parameter.Parameter):
    dropout = torch.nn.functional.dropout(_stack0_0_, p = 0.1, training = False);  _stack0_0_ = None
    add = residual + dropout;  residual = dropout = None
    layer_norm = torch.nn.functional.layer_norm(add, (1024,), self_final_layer_norm_weight, self_final_layer_norm_bias, 1e-05);  self_final_layer_norm_weight = self_final_layer_norm_bias = None
    linear = torch._C._nn.linear(layer_norm, self_fc1_weight, self_fc1_bias);  layer_norm = self_fc1_weight = self_fc1_bias = None
    gelu = torch._C._nn.gelu(linear);  linear = None
    dropout_1 = torch.nn.functional.dropout(gelu, p = 0.0, training = False);  gelu = None
    linear_1 = torch._C._nn.linear(dropout_1, self_fc2_weight, self_fc2_bias);  dropout_1 = self_fc2_weight = self_fc2_bias = None
    dropout_2 = torch.nn.functional.dropout(linear_1, p = 0.1, training = False);  linear_1 = None
    add_1 = add + dropout_2;  add = dropout_2 = None
    return (add_1,)
    
--> GraphModule()



def forward(self, _stack1_0_ : torch.Tensor, self_layer_norm_weight : torch.nn.parameter.Parameter, self_layer_norm_bias : torch.nn.parameter.Parameter):
    layer_norm = torch.nn.functional.layer_norm(_stack1_0_, (1024,), self_layer_norm_weight, self_layer_norm_bias, 1e-05);  _stack1_0_ = self_layer_norm_weight = self_layer_norm_bias = None
    return (layer_norm,)
    
Nuum_graphs 14
PASSED

=============================== warnings summary ===============================
<frozen importlib._bootstrap>:283
  <frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead

tests/test_gradual_types.py: 11 warnings
  /Users/migeedz/opt/anaconda3/envs/pytorch2/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:141: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/migeedz/pytorch/build/aten/src/ATen/core/TensorBody.h:483.)
    if t.grad is not None:

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================ 1 passed, 4 deselected, 12 warnings in 11.46s =================
